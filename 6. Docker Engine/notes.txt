Docker Engine is referred to as a host with Docker installed on it. When you install Docker on a linux
host, you actually install 3 different components: Docker CLI, REST API, Docker Deamon. 

Docker Deamon is a background process that manages docker objects such as the images, containers, volumes
and networks. The Docker REST API server that programs can use to talk to the deamon and provide 
instructions. Docker CLI is nothing but a command line interface which we have been using till now to 
perform actions such as running a container, stoppping containers. Docker CLI uses REST API to interact
with the Docker Deamon. Docker CLI need not necessarily be on the same host. It can be on another 
system like a laptop and can still work with a remote Docker Engine.

To run a container on a remote docker host, run the following command.
Command: docker -H=ipa:port run nginx

---------------------------------------------------------------------
The process of containerization

Docker uses namespace to isolate workspace.

The underlying host as well as the containers share the same system resources as CPU and memory.
By default, there is no restriction as to how much of a resource a container can use and hence a 
container may end up utilizing all of the resources on the underlying host. There is a way to restrict
the amount of CPU or memory a container can use. 

Command: docker run --cpus=.5 ubuntu
Explaination: this command ensures that the container does not take up more than 50% of the host CPU
at any given time.

Command: docker run --memory=100m ubuntu
Explaination: this command ensures that the container can use a maximum of 100MB of memory

-------------------------------------------------------------------------------------
Docker Storage and File System

How does docker stores data on the local file system?

When you install docker on a system, it creates this folder structure at var/lib/docker. You have 
multiple folders under it called aufs, containers, image, volume, etc. This is where docker stores
all its data by default. (data means, files related to images and containers running on the docker host)


Docker Layered Architecture

Docker build images in a layered architecture. Each line of instruction in the docker file creates
a new layer in the docker image with just the changes from the previous layer. If docker is supposed
to create another image from another dockerfile, if it has a particular layer in cache, it will use
it instead of creating a new layer. Similarly, if you happen to update a small change in your application
code, then docker utilizes previous cache to rebuild the image, instead of rebuilding the image from 
scratch. 

Once the image build is complete, you cannot modify the contents of these layers and so there are read
only and you can only modify them by initiating a new build. When you run a container, docker creates
a new writable container layer on top of image layers. The writable layer is used to store data created
by the containers such as log files by the applications, any temporary files generated by the containers
or just any files modified by the user on that container. The life of this layer is only as long as the 
container is alive. When the container is destroyed, this layer and all of the changes stored in it are
also destroyed. If we run multiple containers based off of one image, then all the containers share the
same image layer. 

If we login to the newly created container and create a new file. It would be created in the container
layer which is read and write. Files in the image layer are read only, you cannot edit anything in those
layers.

If we wish to modify the source code of our application. The same image layer may be shared between 
multiple containers created from this image. So does it mean that we cannot modify the file inside this
container? No, we can. We can modify the source code, but before we save the modified file, Docker
automatically creates a copy of the file in the read write layer and we will be modifying the file stored
in read write layer (ie the container layer). All future modifications will be done on this copy of 
read write layer. This is called copy on write mechanism. The image will remain the same, until you
rebuild the image. What happens when we get rid of the container? All of the data that was stored in the 
container layer also gets deleted. What if we wish to persist this data? For example, if we were working
with a database and we would like to preserve the data created by the container, we could add a 
persistent volume to the container. To do this, first create a volume using the docker volume create 
command. 
Command: docker volume create data_volume
Explaination: this command creates a folder 'data_volume' under /var/lib/docker/volumes
Then when we run the container using the docker run command we can mount this volume inside the 
container's read write layer using the following command
Command: docker run -v data_volume:/var/lib/mysql mysql
Explaination: /var/lib/mysql is the default location where mysql store data inside a container.
All data written by the database is stored in the volume created on the docker host. Even if the 
container is destroyed now, the data will persist. If you did not create a volume befor executing
docker run -v command, docker will automatically create a volume for you and mount it to the container.

let's say we have our data at the location /data in the docker host and we would like our container to 
write the data to this location and not in the default /var/lib/docker folder. In this case, we would
provide the complete part of the folder we would like to mount like below
Command: docker run -v /data/mysql:/var/lib/mysql mysql
This is called bind mounting

There are two types of mounts: volume mounting and bind mount. Volume mount mounts a volume from the 
volumes directory and bind mount mounts a directory from any location on the docker host.

It is the responsibility of storage driver to maintain the layered architecture, create a writable 
layer, moving files across layers.

Some common storage drivers are: AUFS, ZFS, BTRFS, Device Mapper, Overlay, Overlay2
The selection of the storage driver depends on the underlying OS used. With Ubuntu, the default storage
driver is AUFS. Docker will automatically choose the best storage driver based on the OS.